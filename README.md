# Esport Big Data Pipeline

A production-ready **Big Data analytics platform** for esports using **Riot Games API**, built with **Apache Kafka**, **Apache Spark Structured Streaming**, and **Docker**. Designed for **GitHub Codespaces** and modular scalability.

---

## ğŸ— Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Riot Games API â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Producers    â”‚â”€â”€â”€â”€â”€â”€>â”‚  Kafka Topics        â”‚
â”‚ (Data Ingestion)   â”‚       â”‚  (Event Streaming)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        v
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Spark Structured Streaming  â”‚
                         â”‚  (Real-time Processing)      â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        v
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Data Lake (Parquet)         â”‚
                         â”‚  Bronze â†’ Silver â†’ Gold      â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                      â”‚
                    v                                      v
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Batch Analytics â”‚                  â”‚  ML Pipelines    â”‚
         â”‚  (Spark Jobs)    â”‚                  â”‚  (Training)      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                                      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        v
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  Visualization      â”‚
                              â”‚  (Streamlit/Grafana)â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Implementation Status

| Phase | Component | Status | Documentation |
|-------|-----------|--------|---------------|
| **Phase 1** | Infrastructure Setup | âœ… Complete | [QUICKSTART.md](QUICKSTART.md) |
| **Phase 2** | Kafka Data Ingestion | âœ… Complete | [INGESTION_GUIDE.md](INGESTION_GUIDE.md) |
| **Phase 3** | Spark Structured Streaming | âœ… Complete | [spark/README.md](spark/README.md) |
| **Phase 4** | Analytics & Aggregations | âœ… Complete | [PHASE4_ANALYTICS.md](PHASE4_ANALYTICS.md) |
| **Phase 5** | Storage & BI Integration | âœ… Complete | [PHASE5_STORAGE.md](PHASE5_STORAGE.md) |
| **Phase 6** | Advanced ML & Visualization | ğŸ”„ Planned | - |

### Phase 5 Highlights (Latest)
- **Storage Systems:** PostgreSQL (5 tables, 11 views) + MongoDB (3 collections)
- **Streamlit Dashboard:** 5 interactive pages (Overview, Players, Teams, Matches, Rankings)
- **BI Optimization:** 15+ indexes, auto-update triggers, window functions
- **JDBC Loaders:** Idempotent Spark-to-PostgreSQL data loading
- **Lines of Code:** 1,800+ lines
- **Documentation:** [Quick Start](PHASE5_QUICKSTART.md) | [Full Docs](PHASE5_STORAGE.md) | [Status](PHASE5_STATUS.md)
- **Dashboard:** http://localhost:8501

---

## ğŸ“ Project Structure

```
esport-bigdata-pipeline/
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json         # GitHub Codespaces configuration
â”œâ”€â”€ conf/
â”‚   â”œâ”€â”€ logging.yaml               # Python logging config
â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â””â”€â”€ topics.yaml            # Kafka topic definitions
â”‚   â””â”€â”€ spark/
â”‚       â””â”€â”€ spark-defaults.conf    # Spark default settings
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Raw ingestion layer
â”‚   â”œâ”€â”€ bronze/                    # Unprocessed data
â”‚   â”œâ”€â”€ silver/                    # Cleaned data
â”‚   â””â”€â”€ gold/                      # Aggregated analytics
â”œâ”€â”€ requirements/
â”‚   â”œâ”€â”€ ingestion.txt              # Kafka producer dependencies
â”‚   â”œâ”€â”€ spark.txt                  # Spark job dependencies
â”‚   â”œâ”€â”€ ml.txt                     # ML pipeline dependencies
â”‚   â”œâ”€â”€ visualization.txt          # Dashboard dependencies
â”‚   â””â”€â”€ dev.txt                    # Development tools
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_topics.sh           # Kafka topic creation script
â”‚   â””â”€â”€ spark-submit-example.sh    # Spark job submission template
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/                    # Shared utilities
â”‚   â”‚   â””â”€â”€ logging_config.py      # Logging setup
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py            # Environment settings
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ riot_producer.py       # Riot API â†’ Kafka producer
â”‚   â”œâ”€â”€ streaming/
â”‚   â”‚   â””â”€â”€ jobs/                  # Spark Structured Streaming jobs
â”‚   â”œâ”€â”€ batch/
â”‚   â”‚   â””â”€â”€ jobs/                  # Spark batch analytics
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â””â”€â”€ pipelines/             # ML training pipelines
â”‚   â”œâ”€â”€ storage/                   # Data lake abstractions
â”‚   â””â”€â”€ visualization/             # Dashboards
â”œâ”€â”€ .env.example                   # Environment template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ docker-compose.yml             # Orchestration of all services
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### Prerequisites

- **GitHub Codespaces** (recommended) or local Docker + Docker Compose
- **Riot Games API Key**: [Get one here](https://developer.riotgames.com/)

### Setup Steps

1. **Clone the repository** (or open in Codespaces):
   ```bash
   git clone https://github.com/louatizine/esport-bigdata-pipeline.git
   cd esport-bigdata-pipeline
   ```

2. **Configure environment**:
   ```bash
   cp .env.example .env
   # Edit .env and set your RIOT_API_KEY
   ```

3. **Start services one-by-one** (see below)

---

## ğŸ³ Docker Commands (Run One-by-One)

### Start Zookeeper
```bash
docker compose --profile core up -d zookeeper
```

**Verify:**
```bash
docker compose logs zookeeper
```

---

### Start Kafka
```bash
docker compose --profile core up -d kafka
```

**Verify:**
```bash
docker compose logs kafka
docker compose exec kafka kafka-broker-api-versions.sh --bootstrap-server kafka:9092
```

---

### Create Kafka Topics (Optional)
```bash
docker compose exec kafka kafka-topics.sh \
  --create \
  --topic esports.matches \
  --bootstrap-server kafka:9092 \
  --partitions 3 \
  --replication-factor 1
```

**List topics:**
```bash
docker compose exec kafka kafka-topics.sh --list --bootstrap-server kafka:9092
```

---

### Start Spark Master
```bash
docker compose --profile spark up -d spark-master
```

**Verify:**
```bash
docker compose logs spark-master
# Spark UI: http://localhost:8080
```

---

### Start Spark Worker
```bash
docker compose --profile spark up -d spark-worker
```

**Verify:**
```bash
docker compose logs spark-worker
# Worker UI: http://localhost:8081
```

---

### Start Optional Services (MongoDB, PostgreSQL)
```bash
docker compose --profile optional up -d mongodb postgres
```

**Verify:**
```bash
docker compose ps
```

---

### Stop All Services
```bash
docker compose --profile core --profile spark --profile optional down
```

**Remove volumes (clean slate):**
```bash
docker compose down -v
```

---

## ğŸ”§ Development Workflow

### Install Python Dependencies
```bash
pip install -r requirements/ingestion.txt
pip install -r requirements/spark.txt
pip install -r requirements/ml.txt
pip install -r requirements/dev.txt
```

### Run a Kafka Producer (Example)
```bash
python src/ingestion/riot_producer.py
```

### Submit a Spark Job (Example)
```bash
docker compose exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  /workspace/src/streaming/jobs/streaming_job_template.py
```

---

## ğŸ§ª Testing & Linting

```bash
# Format code
black src/

# Lint
flake8 src/

# Type checking
mypy src/

# Run tests
pytest
```

---

## ğŸ“Š Data Lake Layers

| Layer    | Description                          | Location         |
|----------|--------------------------------------|------------------|
| **Raw**  | Unprocessed API responses            | `data/raw/`      |
| **Bronze** | Raw ingested events from Kafka     | `data/bronze/`   |
| **Silver** | Cleaned, deduplicated, validated   | `data/silver/`   |
| **Gold**   | Aggregated, business-level metrics | `data/gold/`     |

---

## ğŸ›  Technology Stack

- **Data Streaming**: Apache Kafka + Zookeeper
- **Processing**: Apache Spark (Structured Streaming + Batch)
- **Storage**: Parquet (Data Lake)
- **Orchestration**: Docker Compose
- **Language**: Python 3.11+
- **Optional**: MongoDB, PostgreSQL (persistence)
- **Visualization**: Streamlit / Grafana (TBD)

---

## ğŸ“ Notes

- Keep `.env` out of version control (already in `.gitignore`)
- Use profiles (`core`, `spark`, `optional`) to control which services run
- Data lake directories are Git-tracked via `.gitkeep` but ignored for content
- Spark configs are mounted read-only from `conf/spark/`

---

## ğŸ“œ License

MIT (or adjust per your needs)

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a feature branch
3. Follow code style (black, flake8)
4. Submit a PR

---

**Happy streaming! ğŸš€**