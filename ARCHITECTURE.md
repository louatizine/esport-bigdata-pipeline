# Architecture & Design Patterns

## ğŸ— System Architecture

### High-Level Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          RIOT GAMES API                             â”‚
â”‚                    (Match Data, Player Stats)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ HTTP REST API calls
                             â”‚
                             v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INGESTION LAYER                                â”‚
â”‚                  (Kafka Producers - Python)                         â”‚
â”‚                                                                     â”‚
â”‚  â€¢ riot_producer.py                                                 â”‚
â”‚  â€¢ Fetch data from Riot API                                         â”‚
â”‚  â€¢ Serialize to JSON/Avro                                           â”‚
â”‚  â€¢ Publish to Kafka topics                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ Kafka Messages
                             â”‚
                             v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      STREAMING LAYER                                â”‚
â”‚                    (Apache Kafka + Zookeeper)                       â”‚
â”‚                                                                     â”‚
â”‚  Topics:                                                            â”‚
â”‚  â€¢ esports.matches         (match events)                           â”‚
â”‚  â€¢ esports.player_stats    (player performance)                     â”‚
â”‚  â€¢ esports.game_events     (in-game actions)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ Kafka Consumer API
                             â”‚
                             v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PROCESSING LAYER                                  â”‚
â”‚                (Spark Structured Streaming)                         â”‚
â”‚                                                                     â”‚
â”‚  Streaming Jobs:                                                    â”‚
â”‚  â€¢ Real-time match analytics                                        â”‚
â”‚  â€¢ Player performance scoring                                       â”‚
â”‚  â€¢ Win rate calculations                                            â”‚
â”‚  â€¢ Anomaly detection                                                â”‚
â”‚                                                                     â”‚
â”‚  Batch Jobs:                                                        â”‚
â”‚  â€¢ Daily aggregations                                               â”‚
â”‚  â€¢ Historical trend analysis                                        â”‚
â”‚  â€¢ Leaderboard generation                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ Parquet writes
                             â”‚
                             v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      STORAGE LAYER                                  â”‚
â”‚                    (Data Lake - Parquet)                            â”‚
â”‚                                                                     â”‚
â”‚  Medallion Architecture:                                            â”‚
â”‚                                                                     â”‚
â”‚  [RAW]      â†’ Raw API responses (JSON)                              â”‚
â”‚  [BRONZE]   â†’ Unprocessed events from Kafka                         â”‚
â”‚  [SILVER]   â†’ Cleaned, validated, deduplicated                      â”‚
â”‚  [GOLD]     â†’ Aggregated business metrics                           â”‚
â”‚                                                                     â”‚
â”‚  Optional:                                                          â”‚
â”‚  â€¢ MongoDB (document store for metadata)                            â”‚
â”‚  â€¢ PostgreSQL (relational for user management)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ Reads from Gold layer
                             â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                                    â”‚
          v                                    v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ML PIPELINES       â”‚          â”‚   ANALYTICS LAYER    â”‚
â”‚                      â”‚          â”‚                      â”‚
â”‚ â€¢ Feature engineeringâ”‚          â”‚ â€¢ Dashboards         â”‚
â”‚ â€¢ Model training     â”‚          â”‚ â€¢ Reporting          â”‚
â”‚ â€¢ Predictions        â”‚          â”‚ â€¢ Alerts             â”‚
â”‚ â€¢ A/B testing        â”‚          â”‚                      â”‚
â”‚                      â”‚          â”‚ Tools:               â”‚
â”‚ Tools:               â”‚          â”‚ â€¢ Streamlit          â”‚
â”‚ â€¢ Spark MLlib        â”‚          â”‚ â€¢ Grafana            â”‚
â”‚ â€¢ scikit-learn       â”‚          â”‚ â€¢ Jupyter            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Technology Stack Details

### Orchestration
- **Docker Compose**: Multi-container orchestration
- **Profiles**: `core`, `spark`, `optional` for modular deployment

### Messaging & Streaming
- **Apache Kafka 3.7**: Distributed event streaming platform
- **Zookeeper 3.9**: Kafka coordination service
- **Bitnami Images**: Production-ready, well-maintained containers

### Processing
- **Apache Spark 3.5**: Unified analytics engine
  - Structured Streaming for real-time
  - Batch for historical analysis
  - MLlib for machine learning

### Storage
- **Parquet**: Columnar storage format (optimal for analytics)
- **Data Lake**: Medallion architecture (Bronze â†’ Silver â†’ Gold)
- **Optional DBs**:
  - MongoDB 6: Document store
  - PostgreSQL 15: Relational database

### Development
- **Python 3.11+**: Primary language
- **GitHub Codespaces**: Cloud development environment
- **VS Code**: IDE with Docker/Python extensions

---

## ğŸ“Š Data Models

### Kafka Topics Schema

#### `esports.matches`
```json
{
  "match_id": "string",
  "game_mode": "string",
  "game_duration": "integer",
  "game_start_timestamp": "long",
  "participants": [
    {
      "summoner_name": "string",
      "champion": "string",
      "kills": "integer",
      "deaths": "integer",
      "assists": "integer",
      "team": "string"
    }
  ],
  "winning_team": "string"
}
```

#### `esports.player_stats`
```json
{
  "summoner_id": "string",
  "summoner_name": "string",
  "tier": "string",
  "rank": "string",
  "win_rate": "double",
  "total_games": "integer",
  "timestamp": "long"
}
```

---

## ğŸ—‚ Data Lake Schema

### Bronze Layer
- Raw Kafka events (unchanged)
- Partitioned by: `date`, `hour`
- Format: Parquet with snappy compression

### Silver Layer
- Cleaned and validated data
- Deduplicated by business keys
- Type conversions and standardization
- Partitioned by: `date`, `game_mode`

### Gold Layer
- Aggregated metrics
- Pre-computed KPIs
- Optimized for query performance
- Partitioned by: `date`, `metric_type`

---

## ğŸ”„ Processing Patterns

### Streaming Pattern (Micro-batch)
```python
# Pseudo-code
spark.readStream \
  .format("kafka") \
  .option("kafka.bootstrap.servers", "kafka:9092") \
  .option("subscribe", "esports.matches") \
  .load() \
  .selectExpr("CAST(value AS STRING)") \
  .writeStream \
  .format("parquet") \
  .option("path", "data/bronze/matches") \
  .option("checkpointLocation", "data/checkpoints/matches") \
  .trigger(processingTime="30 seconds") \
  .start()
```

### Batch Pattern (Daily aggregation)
```python
# Pseudo-code
df = spark.read.parquet("data/silver/matches") \
  .filter(col("date") == yesterday) \
  .groupBy("game_mode", "champion") \
  .agg(
    count("*").alias("games_played"),
    avg("win_rate").alias("avg_win_rate")
  ) \
  .write.mode("append") \
  .partitionBy("date") \
  .parquet("data/gold/champion_stats")
```

---

## ğŸ›¡ Design Principles

### 1. **Separation of Concerns**
- Each module has a single responsibility
- `ingestion/` only fetches and produces
- `streaming/` only processes streams
- `batch/` only runs analytics
- `ml/` only handles models

### 2. **Configuration as Code**
- All settings in `.env` or `conf/`
- No hardcoded values in source code
- Environment-driven behavior

### 3. **Idempotency**
- Kafka topics are idempotent-safe
- Batch jobs use `mode("append")` with deduplication
- Checkpoint locations for exactly-once semantics

### 4. **Scalability**
- Horizontal: Add more Spark workers
- Vertical: Adjust memory/cores in `docker-compose.yml`
- Kafka partitions enable parallel consumption

### 5. **Observability**
- Centralized logging (`conf/logging.yaml`)
- Spark UI at http://localhost:8080
- Kafka monitoring via console tools

### 6. **Modularity**
- Docker profiles for optional components
- Separate requirements files per module
- Pluggable ML pipelines

---

## ğŸ” Security Considerations

### Production Checklist
- [ ] Enable Kafka SASL/SSL authentication
- [ ] Use Spark encryption (`spark.ssl.enabled`)
- [ ] Store secrets in secrets manager (not `.env`)
- [ ] Enable network policies/firewall rules
- [ ] Use read-only mounts for configs
- [ ] Implement rate limiting on Riot API calls
- [ ] Add monitoring and alerting (Prometheus + Grafana)

---

## ğŸ“ˆ Scaling Strategies

### Vertical Scaling
```yaml
# In docker-compose.yml
spark-worker:
  environment:
    - SPARK_WORKER_MEMORY=4G
    - SPARK_WORKER_CORES=2
```

### Horizontal Scaling
```bash
# Add more workers
docker compose up -d --scale spark-worker=3
```

### Kafka Partitioning
```bash
# Create topics with more partitions
kafka-topics.sh --create \
  --topic esports.matches \
  --partitions 10 \
  --replication-factor 2
```

---

## ğŸ§ª Testing Strategy

### Unit Tests
- Test individual transformations
- Mock Kafka producers/consumers
- Use `pytest` with fixtures

### Integration Tests
- Test end-to-end flows
- Use Docker Compose for test environment
- Verify data in data lake

### Performance Tests
- Measure throughput (events/second)
- Monitor Spark job execution time
- Profile memory usage

---

**Reference:** This architecture follows the Lambda Architecture pattern with a focus on real-time stream processing complemented by batch reprocessing capabilities.
