# Phase 3: Spark Structured Streaming - Implementation Complete âœ…

## ğŸ“‹ Overview

Phase 3 implements a production-ready **Spark Structured Streaming** pipeline for real-time esports analytics. The system consumes data from Kafka topics, processes streaming events, and writes results to a Parquet-based data lake.

---

## ğŸ¯ Implementation Summary

### âœ… Completed Components

1. **Folder Structure** - Clean, modular organization
2. **Schema Definitions** - Explicit schemas for matches and players
3. **Utility Modules** - Spark session management and structured logging
4. **Streaming Jobs** - Match and player stream processors
5. **Main Orchestrator** - Coordinated execution of multiple streams
6. **Documentation** - Comprehensive README and examples
7. **Helper Scripts** - Easy-to-use runner scripts

---

## ğŸ“ Project Structure

```
spark/
â”œâ”€â”€ main.py                      # Main orchestrator for all streaming jobs
â”œâ”€â”€ README.md                    # Comprehensive documentation
â”œâ”€â”€ .env.example                 # Environment variable template
â”‚
â”œâ”€â”€ streaming/                   # Streaming job implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ match_stream.py         # Kafka â†’ Spark â†’ Parquet (matches)
â”‚   â””â”€â”€ player_stream.py        # Kafka â†’ Spark â†’ Parquet (players)
â”‚
â”œâ”€â”€ schemas/                     # Data schemas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ match_schema.py         # Match data StructType definitions
â”‚   â””â”€â”€ player_schema.py        # Player data StructType definitions
â”‚
â””â”€â”€ utils/                       # Shared utilities
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ spark_session.py        # SparkSession builder & config
    â””â”€â”€ logger.py               # Structured logging utility

requirements/
â””â”€â”€ spark-streaming.txt         # Python dependencies

scripts/
â””â”€â”€ run_spark_streaming.sh      # Shell script for spark-submit
```

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Set Environment Variables

```bash
export KAFKA_BOOTSTRAP_SERVERS="kafka:9092"
export KAFKA_TOPIC_MATCHES="esports-matches"
export KAFKA_TOPIC_PLAYERS="esports-players"
export SPARK_MASTER_URL="local[*]"
export DATA_LAKE_PATH="/workspaces/esport-bigdata-pipeline/data"
export LOG_LEVEL="INFO"
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements/spark-streaming.txt
```

### 3ï¸âƒ£ Run Streaming Jobs

**Option A: Using Python directly**
```bash
cd spark
python main.py --job all
```

**Option B: Using spark-submit**
```bash
./scripts/run_spark_streaming.sh --job all --await
```

**Option C: Individual jobs**
```bash
# Match stream only
python main.py --job matches

# Player stream only
python main.py --job players
```

---

## ğŸ—ï¸ Architecture

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kafka     â”‚
â”‚   Topics    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ JSON Messages
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spark Structured Streaming     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Read from Kafka       â”‚  â”‚
â”‚  â”‚  2. Parse JSON            â”‚  â”‚
â”‚  â”‚  3. Apply Schema          â”‚  â”‚
â”‚  â”‚  4. Transform Data        â”‚  â”‚
â”‚  â”‚  5. Add Timestamps        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ Structured Data
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Data Lake (Parquet)       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  /processed/matches/    â”‚  â”‚
â”‚  â”‚  /processed/players/    â”‚  â”‚
â”‚  â”‚  /checkpoints/          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Interactions

```
main.py
  â”‚
  â”œâ”€â†’ MatchStreamProcessor
  â”‚     â”œâ”€â†’ read_kafka_stream()
  â”‚     â”œâ”€â†’ process_stream()
  â”‚     â””â”€â†’ write_to_parquet()
  â”‚
  â””â”€â†’ PlayerStreamProcessor
        â”œâ”€â†’ read_kafka_stream()
        â”œâ”€â†’ process_stream()
        â””â”€â†’ write_to_parquet()
```

---

## ğŸ“Š Features Implementation

### Match Stream Processor (`match_stream.py`)

**Input:** `KAFKA_TOPIC_MATCHES` (JSON messages)

**Processing:**
- âœ… Parse JSON with explicit schema
- âœ… Extract match metadata (ID, tournament, teams)
- âœ… Calculate duration in minutes
- âœ… Add processing timestamp
- âœ… Flag completed vs live matches
- âœ… Handle malformed records safely

**Output:** Parquet files partitioned by `status`
```
data/processed/matches/
â”œâ”€â”€ status=finished/
â”œâ”€â”€ status=live/
â””â”€â”€ status=scheduled/
```

**Key Features:**
- Earliest offset consumption
- 10-second micro-batches
- Automatic checkpointing
- Schema enforcement
- Graceful error handling

---

### Player Stream Processor (`player_stream.py`)

**Input:** `KAFKA_TOPIC_PLAYERS` (JSON messages)

**Processing:**
- âœ… Parse JSON with explicit schema
- âœ… Extract player profile data
- âœ… Calculate derived metrics (KDA, win rate)
- âœ… Enrich with full name and status
- âœ… Add activity flags
- âœ… Handle missing values

**Output:** Parquet files partitioned by `status` and `role`
```
data/processed/players/
â”œâ”€â”€ status=active/
â”‚   â”œâ”€â”€ role=Top/
â”‚   â”œâ”€â”€ role=Jungle/
â”‚   â”œâ”€â”€ role=Mid/
â”‚   â”œâ”€â”€ role=ADC/
â”‚   â””â”€â”€ role=Support/
â””â”€â”€ status=inactive/
```

**Key Features:**
- Performance metric calculations
- Multi-level partitioning
- Null-safe transformations
- Automatic enrichment

---

### Spark Session Builder (`spark_session.py`)

**Capabilities:**
- âœ… Kafka SQL connector integration
- âœ… Optimized streaming configurations
- âœ… Docker-compatible settings
- âœ… Memory management (driver/executor)
- âœ… Kryo serialization
- âœ… Graceful shutdown support
- âœ… Configurable UI port

**Key Configurations:**
```python
{
    "spark.streaming.stopGracefullyOnShutdown": "true",
    "spark.sql.streaming.schemaInference": "false",
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
    "spark.sql.shuffle.partitions": "10",
    "spark.checkpoint.compress": "true"
}
```

---

### Structured Logger (`logger.py`)

**Features:**
- âœ… Consistent formatting across all components
- âœ… Configurable log levels via `LOG_LEVEL` env var
- âœ… Context-aware logging with key-value pairs
- âœ… Timestamp inclusion
- âœ… Module-level logger instances

**Example Output:**
```
2025-12-31 14:30:45 | match_stream | INFO | Starting match streaming pipeline
2025-12-31 14:30:46 | match_stream | INFO | Successfully connected to Kafka stream
2025-12-31 14:30:47 | match_stream | INFO | Streaming query started | query_name=match_stream | query_id=abc123
```

---

### Main Orchestrator (`main.py`)

**Capabilities:**
- âœ… Run individual or all streaming jobs
- âœ… Shared SparkSession for efficiency
- âœ… Signal handling (SIGINT/SIGTERM)
- âœ… Graceful shutdown
- âœ… Query monitoring
- âœ… Command-line arguments

**CLI Options:**
```bash
--job {matches|players|all}    # Which job(s) to run
--await-termination             # Wait for query termination
```

**Usage Examples:**
```bash
# Run all jobs and monitor
python main.py --job all

# Run matches only and await
python main.py --job matches --await-termination

# Run players only
python main.py --job players
```

---

## ğŸ”§ Configuration

### Environment Variables Reference

| Variable | Purpose | Default | Required |
|----------|---------|---------|----------|
| `KAFKA_BOOTSTRAP_SERVERS` | Kafka broker addresses | - | âœ… Yes |
| `KAFKA_TOPIC_MATCHES` | Match events topic | `esports-matches` | No |
| `KAFKA_TOPIC_PLAYERS` | Player events topic | `esports-players` | No |
| `SPARK_MASTER_URL` | Spark master URL | `local[*]` | No |
| `DATA_LAKE_PATH` | Base data directory | `/workspaces/.../data` | No |
| `PROCESSED_DATA_PATH` | Processed data dir | `{DATA_LAKE_PATH}/processed` | No |
| `LOG_LEVEL` | Logging verbosity | `INFO` | No |
| `SPARK_DRIVER_MEMORY` | Driver memory | `2g` | No |
| `SPARK_EXECUTOR_MEMORY` | Executor memory | `2g` | No |
| `SPARK_SHUFFLE_PARTITIONS` | Shuffle parallelism | `10` | No |
| `SPARK_UI_PORT` | Spark web UI port | `4040` | No |

---

## ğŸ“ Schema Definitions

### Match Schema Fields

```python
{
    "match_id": "string",              # Primary key
    "tournament_name": "string",       # Tournament identifier
    "team_1_name": "string",           # Team A
    "team_2_name": "string",           # Team B
    "winner_name": "string",           # Match winner
    "match_duration": "integer",       # Duration in seconds
    "team_1_kills": "integer",         # Team A kills
    "team_2_kills": "integer",         # Team B kills
    "status": "string",                # finished/live/scheduled
    "started_at": "timestamp",         # Match start time
    "finished_at": "timestamp",        # Match end time
    "processing_timestamp": "timestamp" # ETL timestamp
}
```

### Player Schema Fields

```python
{
    "player_id": "string",             # Primary key
    "summoner_name": "string",         # In-game name
    "current_team_name": "string",     # Team affiliation
    "role": "string",                  # Position (Top/Jungle/Mid/ADC/Support)
    "total_games": "integer",          # Career games
    "total_wins": "integer",           # Career wins
    "win_rate": "double",              # Win percentage
    "avg_kills": "double",             # Average kills per game
    "avg_deaths": "double",            # Average deaths per game
    "avg_assists": "double",           # Average assists per game
    "kda_ratio": "double",             # KDA metric
    "active": "boolean",               # Player status
    "processing_timestamp": "timestamp" # ETL timestamp
}
```

---

## ğŸ›¡ï¸ Error Handling

### Implemented Safeguards

1. **Malformed JSON**
   - Logged as errors
   - Skipped gracefully
   - No job failures

2. **Missing Fields**
   - Schema allows nulls
   - Default values applied
   - Downstream validation

3. **Kafka Connection Failures**
   - Detailed error logging
   - Automatic retries (Kafka client)
   - Fail-fast on startup

4. **Write Failures**
   - Checkpointing ensures recovery
   - Exactly-once semantics
   - No data loss

5. **Graceful Shutdown**
   - SIGINT/SIGTERM handling
   - Checkpoint flushing
   - Query cleanup

---

## ğŸ“ˆ Monitoring & Observability

### Spark UI
Access the Spark Web UI at:
```
http://localhost:4040
```

**Available Tabs:**
- Jobs (execution progress)
- Stages (task details)
- Storage (cached data)
- Streaming (query metrics)
- SQL (query plans)

### Log Monitoring

**Key Log Messages:**
```log
# Startup
INFO | Creating SparkSession for application: EsportsAnalytics_Streaming
INFO | Starting match streaming pipeline
INFO | Streaming query started | query_name=match_stream

# Runtime
INFO | Query match_stream status | is_active=True | is_data_available=True

# Errors
ERROR | Failed to read from Kafka: Connection refused
ERROR | Query match_stream has stopped unexpectedly
```

### Query Metrics

Monitor via `StreamingQuery.status`:
```python
{
    "isActive": true,
    "isDataAvailable": true,
    "message": "Processing new data",
    "isTriggerActive": false,
    "isWatermarkPresent": false
}
```

---

## ğŸ§ª Testing

### Manual Testing

1. **Start services**
   ```bash
   # Start Kafka (Phase 2)
   docker-compose up -d kafka
   ```

2. **Produce test data**
   ```bash
   python scripts/test_kafka_producer.py
   ```

3. **Run streaming job**
   ```bash
   cd spark
   python main.py --job all
   ```

4. **Verify output**
   ```bash
   ls -lh data/processed/matches/
   ls -lh data/processed/players/
   ```

5. **Read Parquet files**
   ```python
   from pyspark.sql import SparkSession

   spark = SparkSession.builder.appName("test").getOrCreate()

   # Read matches
   matches_df = spark.read.parquet("data/processed/matches")
   matches_df.show(10, truncate=False)

   # Read players
   players_df = spark.read.parquet("data/processed/players")
   players_df.show(10, truncate=False)
   ```

---

## ğŸ”’ Production Readiness

### âœ… Production Features

- [x] No hardcoded values (all configuration via env vars)
- [x] Comprehensive error handling
- [x] Graceful shutdown mechanisms
- [x] Checkpointing for fault tolerance
- [x] Structured logging
- [x] Schema enforcement
- [x] Partitioned output
- [x] Modular, maintainable code
- [x] Extensive documentation
- [x] CLI support

### ğŸ¯ Production Recommendations

1. **Resource Tuning**
   ```bash
   export SPARK_DRIVER_MEMORY="4g"
   export SPARK_EXECUTOR_MEMORY="4g"
   export SPARK_SHUFFLE_PARTITIONS="50"
   ```

2. **Monitoring Integration**
   - Connect Spark metrics to Prometheus
   - Set up Grafana dashboards
   - Configure alerting (PagerDuty, Slack)

3. **Checkpoint Storage**
   - Use HDFS, S3, or Azure Blob Storage
   - Enable checkpoint compression
   - Regular cleanup of old checkpoints

4. **Backpressure Control**
   ```python
   .option("maxOffsetsPerTrigger", 10000)
   .trigger(processingTime="30 seconds")
   ```

5. **Security**
   - Enable Kafka SSL/SASL
   - Configure Spark authentication
   - Use IAM roles for cloud storage

---

## ğŸ“š Additional Resources

### Created Files

1. `spark/main.py` - Main orchestrator
2. `spark/streaming/match_stream.py` - Match processor
3. `spark/streaming/player_stream.py` - Player processor
4. `spark/schemas/match_schema.py` - Match schema
5. `spark/schemas/player_schema.py` - Player schema
6. `spark/utils/spark_session.py` - Spark session builder
7. `spark/utils/logger.py` - Structured logger
8. `spark/README.md` - Detailed documentation
9. `spark/.env.example` - Environment template
10. `requirements/spark-streaming.txt` - Python dependencies
11. `scripts/run_spark_streaming.sh` - Runner script
12. `PHASE3_SPARK_STREAMING.md` - This file

### References

- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Spark-Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)
- [PySpark API](https://spark.apache.org/docs/latest/api/python/)

---

## âœ… Phase 3 Checklist

- [x] Clean folder structure created
- [x] SparkSession configured for streaming + Kafka
- [x] Kafka topic consumption implemented
- [x] Explicit schemas defined
- [x] JSON parsing implemented
- [x] Data transformations applied
- [x] Processing timestamps added
- [x] Parquet output format
- [x] Append mode configured
- [x] Checkpointing enabled
- [x] Structured logging (INFO, ERROR)
- [x] Production-ready code
- [x] Modular architecture
- [x] Well-commented code
- [x] No hardcoded values
- [x] Comprehensive documentation
- [x] Helper scripts
- [x] Environment examples

---

## ğŸ‰ Summary

**Phase 3 is complete!** The Spark Structured Streaming implementation provides:

- âœ… Real-time processing of esports events
- âœ… Fault-tolerant streaming with checkpointing
- âœ… Scalable architecture for high-volume data
- âœ… Clean, maintainable, production-ready code
- âœ… Comprehensive monitoring and logging
- âœ… Flexible configuration via environment variables
- âœ… Easy deployment with Docker compatibility

**Next Steps:**
- Integrate with ML pipelines (Phase 4)
- Add data quality checks
- Implement advanced aggregations
- Create visualization dashboards
- Set up automated testing

---

**Created:** December 31, 2025
**Status:** âœ… Complete
**Version:** 1.0.0
